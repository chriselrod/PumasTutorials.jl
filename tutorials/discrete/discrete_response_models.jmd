---
title: Discrete Response Models
author: Vijay Ivaturi, Chris Rackauckas
date: July 19th, 2019
---

# Introduction

In this tutorial we will go over the simulation of discrete responses. Many
pharmacometrics scenarios have observables, such as pain scores or counts,
which necessarily have to be discrete. Handling this discreteness can be
paramount to getting an appropriate data fit and to properly understand the
variation.

Luckily, in Pumas, discrete outputs are handled no differently from the rest
of the Pumas toolchain. In Pumas, to have a discrete distribution as output,
simply have that your `derived` or `observed` variables come from a discrete
distribution like a `Poisson` process.

## Binary Response Example

First, let's take a look at a binary response. A binary response is a model
which gives an output of 0 or 1 with a probability `p`. In Pumas, this is
represented by a `Bernoulli(p)` distribution.

To get started, first let's load `Pumas` and read in some example data:

```julia
using Pumas, StatsFuns, CSV
data = read_pumas(example_data("pain_remed"),
    cvs = [:arm, :dose, :conc, :painord,:remed],event_data=false)
```

Next let's implement a model with a binary response. Here, we do not have an
`@dynamics` porition. Pumas will automatically handle this. In our `derived`,
we define a `logistic` from [StatsFuns](https://github.com/JuliaStats/StatsFuns.jl#basic-functions)

```julia
binary_model = @model begin
    @param begin
        intercept ∈ RealDomain(init=0.001)
        tvslope ∈ RealDomain(init=0.0001)
        Ω ∈ VectorDomain(1)
    end

    @random begin
        η ~ MvNormal(Ω)
    end

    @covariates arm dose

    @pre begin
        rx = dose > 0 ? 1 : 0
        slope = tvslope*rx
        logit = intercept + slope + η[1]
    end

    @derived begin
        pain = @. logistic(logit)
        dv ~ @. Bernoulli(logistic(logit))
    end
end
```

Note that we could have alternatively defined our `@pre` like:

```{julia;eval=false}
@pre begin
    logit = intercept + tvslope*(dose > 0 ? 1 : 0) + η[1]
    logit = intercept + tvslope*Int(dose > 0) + η[1]
end
```

more directly instead of using `logistic`.

Now let's fit our model to the data:

```julia
param = (
    intercept = 0.001,
    tvslope = 0.0001,
    Ω = [1.0]
    )
```

Below you can fit the model

```julia
binary_res = fit(binary_model,data,param,Pumas.FOCE())
coeftable(binary_res)
```

and make some inferences

```julia
infer(binary_res)  |> coeftable
```

and simulate some outputs:

```julia
sim = simobs(binary_model,data,coef(binary_res))
simdf = DataFrame(sim, include_events=false)
first(simdf,6) # Print only the first 6 rows
```

Note that now our simulation output for `dv` is true/false values pulled with
probability given by `logit` dependent on the individual's random effects.

## Poisson Response Example

Next let's use a `Poisson` counting process in our model. Here we generate a
population where everyone is receiving the same doses as a covariate.

```julia
pop = Population(map(i -> Subject(id=i,cvs=(dose = i.*10,),time=[0.0]),1:10))
```

Now we define our model without dynamics, and directly use the dose information
to predict the count for some observable `dv`:

```julia
poisson_model = @model begin
  @param begin
    tvbase ∈ RealDomain(init=3.0, lower=0.1)
    d50 ∈ RealDomain(init=50, lower=0.1)
    Ω  ∈ PSDDomain(fill(0.1, 1, 1))
  end

  @random begin
    η ~ MvNormal(Ω)
  end

 @covariates dose

  @pre begin
    _dose = dose
    baseline = tvbase*exp(η[1])
  end

  @derived begin
    dv ~ @. Poisson(baseline*(1-_dose/(_dose + d50)))
  end
end
```

and simulate runs from the model:

```julia
sim = simobs(poisson_model,pop)
simdf = DataFrame(sim, include_events=false)
```

Here `dv` is an integer output probabilistically dependent on the dose.

Let's read the data back in to re-estimate the model parameters

```julia
poisson_pop = read_pumas(simdf, dvs=[:dv], cvs=[:dose], event_data=false)
```

```julia
poisson_res = fit(poisson_model,poisson_pop, init_param(poisson_model),Pumas.FOCE())
coeftable(poisson_res)
```

## Negative Binomial Example

Next let's use a `NegativeBinomial` counting process in our model. We will use an internal dataset
as an example.

```julia
pd_poisson = read_pumas(example_data("sim_poisson"), cvs = [:dose], event_data=false)
```

Now we define our model without dynamics, and directly use the dose information
to predict the count for some observable `dv`:

```julia
negativebinomial_model = @model begin
  @param begin
    θ₁ ∈ RealDomain(init=3.0, lower=0.1)
    θ₂ ∈ RealDomain(init=0.5, lower=0.1)
    ω  ∈ RealDomain(init=1.0, lower=0.0)
    θr  ∈ RealDomain(init=1.0, lower=0.0)
  end

  @random begin
    η ~ Normal(0.0, ω)
  end

  @pre begin
    baseline = θ₁*exp(η[1])
    d50 = θ₂
    dose_d50 = dose/(dose+d50)
    r = θr
  end

  @covariates dose

  @vars begin
    m = baseline*(1 - dose_d50)
    p = r/(m + r)
  end

  @derived begin
    dv ~ @. NegativeBinomial(r, p)
  end
end

param = init_param(negativebinomial_model)
```

and simulate runs from the model:

```julia
sim_negativebinomial = simobs(negativebinomial_model, pd_poisson, param; ensemblealg = EnsembleSerial())
```

Here `dv` is an integer output probabilistically dependent on the dose.

Let's read the data back in to re-estimate the model parameters


```julia
pd_negativebinomial  = Subject.(sim_negativebinomial)
```

And fit the data

```julia
ngebin_res = fit(negativebinomial_model, pd_negativebinomial, param, Pumas.FOCE())
coeftable(ngebin_res)
```

and make an inference

```julia
infer(ngebin_res)  |> coeftable
```

## Ordinal data example

Next, we look at a simple example for ordinal data. Again, we will use
an internal dataset

```julia
df = copy(CSV.read(example_data("pain_remed")))
```
The  dependent variable is coded 0:3 but `Categorical` distribution in julia
starts indexing at 1

```julia
df.painord .+= 1
```
Now, that we made this change, lets read the data into Pumas

```julia
data = read_pumas(df,
  dvs = [:painord],
  cvs = [:arm, :dose, :conc, :painord,:remed],
  event_data=false)
```

the ordinal model is below

```julia
ordinal_model = @model begin
  @param begin
    b₁    ∈ RealDomain(init=2.90692)
    b₂    ∈ RealDomain(init=-2.97771, lower=-1000000, upper=1)
    b₃    ∈ RealDomain(init=-2.7541 , lower=-1000000, upper=1)
    slope ∈ RealDomain(init=0.01)
    ω     ∈ RealDomain(init=sqrt(3.10532), lower = 0.001)
  end

  @random begin
    η ~ Normal(0.0, ω)
  end

  @covariates conc

  @pre begin
    effect = slope * conc
    #Logit of cumulative probabilities
    lge₀ = @. b₁ + η + effect
    lge₁ = @. lge₀ + b₂
    lge₂ = @. lge₁ + b₃

    #Probabilities of >=0 and >=1 and >=2
    pge₀ = @. exp(lge₀) / (1.0 + exp(lge₀))
    pge₁ = @. exp(lge₁) / (1.0 + exp(lge₁))
    pge₂ = @. exp(lge₂) / (1.0 + exp(lge₂))

    #Probabilities of Y=0,1,2,3
    p₀ = @. 1.0 - pge₀
    p₁ = @. pge₀ - pge₁
    p₂ = @. pge₁ - pge₂
    p₃ = @. pge₂
  end

  @derived begin
    painord ~ @. Categorical(p₀, p₁, p₂, p₃)
  end
end
```

```julia
ordinal_res = fit(ordinal_model, data, init_param(ordinal_model), Pumas.FOCE())
coeftable(ordinal_res)
```

and get the paremter precision with

```julia
ordinal_res |> infer |> coeftable
```

```julia{echo=false,skip="notebook"}
using PumasTutorials
PumasTutorials.tutorial_footer(WEAVE_ARGS[:folder],WEAVE_ARGS[:file])
```
